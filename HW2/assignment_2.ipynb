{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f6805410",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "from PIL import Image\n",
    "from skimage.color import rgb2gray\n",
    "from sklearn import svm\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import metrics\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf67fee",
   "metadata": {},
   "source": [
    "## PART 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9279199a",
   "metadata": {},
   "source": [
    "##### Helper functions for grouping pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ac28ab41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_pixels(img, bin_number):\n",
    "    max_width = np.max(img) - np.min(img)\n",
    "    bins = []\n",
    "    for i in range(1, bin_number):\n",
    "        bins.append(math.floor(i * (max_width / bin_number)))\n",
    "    bins.append(np.max(img) + 1)\n",
    "    return bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "83ce569e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_level(pix, bins):\n",
    "    group_num = 0\n",
    "    for binn in bins:\n",
    "        if binn > pix:\n",
    "            return group_num\n",
    "        group_num += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d12bd63",
   "metadata": {},
   "source": [
    "#### Functions for Calculation of Cooccurence Matrix and Haralick features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e105b90a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculateCooccurenceMatrix(grayImg, binNumber, di, dj):\n",
    "    refined = grayImg * 256\n",
    "    bins = group_pixels(refined, binNumber)\n",
    "    co_occur_mtx = np.zeros((binNumber, binNumber))\n",
    "    \n",
    "    for i in range(len(refined)):\n",
    "        new_i = i + di\n",
    "        if new_i < 0 or new_i >= len(refined):\n",
    "            continue\n",
    "        for j in range(len(refined[0])):\n",
    "            new_j = j + dj\n",
    "            if new_j < 0 or new_j >= len(refined[0]):\n",
    "                continue\n",
    "            initial = find_level(refined[i, j], bins)\n",
    "            end = find_level(refined[new_i, new_j], bins)\n",
    "            co_occur_mtx[initial, end] += 1\n",
    "    \n",
    "    return co_occur_mtx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "11c8bff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculateAccumulatedCooccurenceMatrix(grayImg, binNumber, d):\n",
    "    connectivity_list = [(d, 0), (d, d), (0, d), (-d, d), (-d, 0), (-d, -d), (0, -d), (d, -d)]\n",
    "    accM = np.zeros((binNumber, binNumber))\n",
    "    for elem in connectivity_list:\n",
    "        accM += calculateCooccurenceMatrix(grayImg, binNumber, elem[0], elem[1])\n",
    "    return accM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "454df27e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculateCooccurrenceFeatures(accM):\n",
    "    ##noramlization\n",
    "    normalized = accM/accM.sum()\n",
    "    \n",
    "    ang_sc_mom = np.square(normalized).sum()\n",
    "    max_prob = np.max(normalized)\n",
    "    \n",
    "    mean_i = np.sum(normalized, axis = 0).mean()\n",
    "    mean_j = np.sum(normalized, axis = 1).mean()\n",
    "    std_i = np.sum(normalized, axis = 0).std()\n",
    "    std_j = np.sum(normalized, axis = 1).std()\n",
    "    \n",
    "    inv_diff_mom = 0\n",
    "    contrast = 0\n",
    "    entropy = 0\n",
    "    corr = 0\n",
    "    \n",
    "    for i in range(len(accM)):\n",
    "        for j in range(len(accM)):\n",
    "            inv_diff_mom += normalized[i, j] / (1 + (i-j)*(i-j))\n",
    "            contrast += (i-j)*(i-j)*normalized[i, j]\n",
    "            if normalized[i, j] != 0:\n",
    "                entropy -= normalized[i,j]*math.log(normalized[i,j])\n",
    "            corr += i*j*normalized[i,j]\n",
    "    \n",
    "    corr = (corr - mean_i*mean_j)/(std_i*std_j)\n",
    "    \n",
    "    ## returned in a dictionary data structure to ease of readability\n",
    "    return {'Angular Second Moment': ang_sc_mom,\n",
    "            'Maximum Probability': max_prob,\n",
    "            'Inverse Difference Moment': inv_diff_mom,\n",
    "            'Contrast': contrast,\n",
    "            'Entropy': entropy,\n",
    "            'Correlation': corr}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e93fe06",
   "metadata": {},
   "source": [
    "## Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07594bf9",
   "metadata": {},
   "source": [
    "#### In the below cell, haralick features for all training images are obtained (class informations and features are kept in a dataframe for ease of manupilation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "id": "b5273567",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "labels = np.loadtxt(\"./dataset/training_labels.txt\")\n",
    "##list for class-balancing issues\n",
    "df['labels'] = labels\n",
    "ans = []\n",
    "for i in range(1,187):\n",
    "    name = 'tr' + str(i) + '.jpg'\n",
    "    image = Image.open(\"./dataset/training\" + '/' + name)\n",
    "    img_data = np.asarray(image)\n",
    "    gray = rgb2gray(img_data)\n",
    "    coor = calculateCooccurrenceFeatures(calculateAccumulatedCooccurenceMatrix(gray,8,10))\n",
    "    ans.append(coor)\n",
    "\n",
    "df['Angular Second Moment'] = [sub['Angular Second Moment'] for sub in ans]\n",
    "df['Maximum Probability'] = [sub['Maximum Probability'] for sub in ans]\n",
    "df['Inverse Difference Moment'] = [sub['Inverse Difference Moment'] for sub in ans]\n",
    "df['Contrast'] = [sub['Contrast'] for sub in ans]\n",
    "df['Entropy'] = [sub['Entropy'] for sub in ans]\n",
    "df['Correlation'] = [sub['Correlation'] for sub in ans]\n",
    "##normalization\n",
    "for column in df.columns:\n",
    "    if column == 'labels':\n",
    "        continue\n",
    "    df[column] = (df[column] - df[column].mean())/df[column].std()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc12177",
   "metadata": {},
   "source": [
    "#### Dataframe example for training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c080a17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a09c1eda",
   "metadata": {},
   "source": [
    "#### In this cell Synthetic Minority Oversampling Technique (SMOTE) is used to solve the imbalanced class problem\n",
    "#### number instances for classes before [1.0: 60, 2.0: 88, 3.0: 38] -> [1.0: 88, 2.0: 88, 3.0: 88]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "id": "0c5784b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [col for col in df.columns if col != 'labels']\n",
    "train_y = df['labels']\n",
    "train_X = df[features]\n",
    "sampler = SMOTE(sampling_strategy='auto')\n",
    "X_over, y_over = sampler.fit_resample(train_X, train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06997df2",
   "metadata": {},
   "source": [
    "#### same process for test data as for training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "id": "34d5008b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.DataFrame()\n",
    "labels = np.loadtxt(\"./dataset/test_labels.txt\")\n",
    "##list for class-balancing issues\n",
    "df2['labels'] = labels\n",
    "ans = []\n",
    "for i in range(1,145):\n",
    "    name = 'ts' + str(i) + '.jpg'\n",
    "    image = Image.open(\"./dataset/test\" + '/' + name)\n",
    "    img_data = np.asarray(image)\n",
    "    gray = rgb2gray(img_data)\n",
    "    coor = calculateCooccurrenceFeatures(calculateAccumulatedCooccurenceMatrix(gray,8,10))\n",
    "    ans.append(coor)\n",
    "\n",
    "df2['Angular Second Moment'] = [sub['Angular Second Moment'] for sub in ans]\n",
    "df2['Maximum Probability'] = [sub['Maximum Probability'] for sub in ans]\n",
    "df2['Inverse Difference Moment'] = [sub['Inverse Difference Moment'] for sub in ans]\n",
    "df2['Contrast'] = [sub['Contrast'] for sub in ans]\n",
    "df2['Entropy'] = [sub['Entropy'] for sub in ans]\n",
    "df2['Correlation'] = [sub['Correlation'] for sub in ans]\n",
    "##normalization\n",
    "for column in df.columns:\n",
    "    if column == 'labels':\n",
    "        continue\n",
    "    df2[column] = (df2[column] - df2[column].mean())/df2[column].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "id": "48ee8529",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_y = df2['labels']\n",
    "test_X = df2[features]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a135109d",
   "metadata": {},
   "source": [
    "#### in the below training process for SVM with linear kernel and results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 692,
   "id": "b7c3769d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[71  8  9]\n",
      " [ 8 70 10]\n",
      " [ 5  6 77]]\n",
      "[0.80681818 0.79545455 0.875     ]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.85      0.81      0.83        88\n",
      "         2.0       0.83      0.80      0.81        88\n",
      "         3.0       0.80      0.88      0.84        88\n",
      "\n",
      "    accuracy                           0.83       264\n",
      "   macro avg       0.83      0.83      0.83       264\n",
      "weighted avg       0.83      0.83      0.83       264\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## For tuning purposes \n",
    "\n",
    "#params = {'C': np.arange(100, 10010, 1000)}\n",
    "# grid = GridSearchCV(estimator=linear_kernel, param_grid=params)\n",
    "# grid.fit(X_over, y_over)\n",
    "#print(grid.best_estimator_)\n",
    "\n",
    "\n",
    "## Support vector machine with linear kernel training and results\n",
    "linear_kernel = svm.SVC(C=1400, kernel='linear')\n",
    "linear_kernel.fit(X_over, y_over)\n",
    "linear_pred = linear_kernel.predict(X_over)\n",
    "mat = metrics.confusion_matrix(y_over, linear_pred)\n",
    "print(mat)\n",
    "print(mat.diagonal()/mat.sum(axis=1))\n",
    "print(metrics.classification_report(y_over, linear_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6def18b7",
   "metadata": {},
   "source": [
    "#### Same process for Radial basis kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 693,
   "id": "ddaefc9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.875      0.875      0.90909091]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.87      0.88      0.87        88\n",
      "         2.0       0.89      0.88      0.88        88\n",
      "         3.0       0.91      0.91      0.91        88\n",
      "\n",
      "    accuracy                           0.89       264\n",
      "   macro avg       0.89      0.89      0.89       264\n",
      "weighted avg       0.89      0.89      0.89       264\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rbf_kernel = svm.SVC(C=7000, gamma=0.015, kernel='rbf')\n",
    "rbf_kernel.fit(X_over, y_over)\n",
    "rbf_pred = rbf_kernel.predict(X_over)\n",
    "mat = metrics.confusion_matrix(y_over, rbf_pred)\n",
    "print(mat.diagonal()/mat.sum(axis=1))\n",
    "print(metrics.classification_report(y_over, rbf_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da4aaa1",
   "metadata": {},
   "source": [
    "#### Creation of contingency matrices for all classes and Mc nemar's test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 694,
   "id": "6747ec5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[69, 2], [8, 9]]\n",
      "[[70, 0], [7, 11]]\n",
      "[[71, 6], [9, 2]]\n"
     ]
    }
   ],
   "source": [
    "contingency_1 = [[0,0], [0,0]]\n",
    "contingency_2 = [[0,0], [0,0]]\n",
    "contingency_3 = [[0,0], [0,0]]\n",
    "for i in range(len(y_over)):\n",
    "    if y_over[i] == 1.0:\n",
    "        if y_over[i] == linear_pred[i] and y_over[i] == rbf_pred[i]:\n",
    "            contingency_1[0][0] += 1\n",
    "        elif y_over[i] == linear_pred[i] and y_over[i] != rbf_pred[i]:\n",
    "            contingency_1[0][1] += 1\n",
    "        elif y_over[i] != linear_pred[i] and y_over[i] == rbf_pred[i]:\n",
    "            contingency_1[1][0] += 1\n",
    "        else: \n",
    "            contingency_1[1][1] += 1\n",
    "    elif y_over[i] == 2.0:\n",
    "        if y_over[i] == linear_pred[i] and y_over[i] == rbf_pred[i]:\n",
    "            contingency_2[0][0] += 1\n",
    "        elif y_over[i] == linear_pred[i] and y_over[i] != rbf_pred[i]:\n",
    "            contingency_2[0][1] += 1\n",
    "        elif y_over[i] != linear_pred[i] and y_over[i] == rbf_pred[i]:\n",
    "            contingency_2[1][0] += 1\n",
    "        else: \n",
    "            contingency_2[1][1] += 1\n",
    "    else:\n",
    "        if y_over[i] == linear_pred[i] and y_over[i] == rbf_pred[i]:\n",
    "            contingency_3[0][0] += 1\n",
    "        elif y_over[i] == linear_pred[i] and y_over[i] != rbf_pred[i]:\n",
    "            contingency_3[0][1] += 1\n",
    "        elif y_over[i] != linear_pred[i] and y_over[i] == rbf_pred[i]:\n",
    "            contingency_3[1][0] += 1\n",
    "        else: \n",
    "            contingency_3[1][1] += 1\n",
    "        \n",
    "print(contingency_1)\n",
    "print(contingency_2)\n",
    "print(contingency_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 698,
   "id": "2cf36715",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pvalue      0.02334220201289086\n",
      "statistic   5.142857142857143\n"
     ]
    }
   ],
   "source": [
    "from statsmodels.stats.contingency_tables import mcnemar\n",
    "print(mcnemar(contingency_2, exact=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e87b783",
   "metadata": {},
   "source": [
    "## Part 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3259d01",
   "metadata": {},
   "source": [
    "#### Function to obtain Haralick features for grid-based approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 593,
   "id": "1164e2ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_based_features(img, binNumber, d, N):\n",
    "    lenx = len(img)\n",
    "    leny = len(img[0])\n",
    "    ans = []\n",
    "    for i in range(N):\n",
    "        for j in range(N):\n",
    "            grid = img[int(lenx * i / N):int(lenx * (i + 1) / N), int(leny * j / N):int(leny * (j + 1) / N)]\n",
    "            feature = calculateCooccurrenceFeatures(calculateAccumulatedCooccurenceMatrix(grid, binNumber, d))\n",
    "            ans.append(feature)\n",
    "    return {'Angular Second Moment': np.array([sub['Angular Second Moment'] for sub in ans]).mean(),\n",
    "            'Maximum Probability': np.array([sub['Maximum Probability'] for sub in ans]).mean(),\n",
    "            'Inverse Difference Moment': np.array([sub['Inverse Difference Moment'] for sub in ans]).mean(),\n",
    "            'Contrast': np.array([sub['Contrast'] for sub in ans]).mean(),\n",
    "            'Entropy': np.array([sub['Entropy'] for sub in ans]).mean(),\n",
    "            'Correlation': np.array([sub['Correlation'] for sub in ans]).mean()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83a1e213",
   "metadata": {},
   "source": [
    "#### Features with grid-based approach for training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 594,
   "id": "02f1514a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = pd.DataFrame()\n",
    "labels = np.loadtxt(\"./dataset/training_labels.txt\")\n",
    "##list for class-balancing issues\n",
    "df3['labels'] = labels\n",
    "ans = []\n",
    "for i in range(1,187):\n",
    "    name = 'tr' + str(i) + '.jpg'\n",
    "    image = Image.open(\"./dataset/training\" + '/' + name)\n",
    "    img_data = np.asarray(image)\n",
    "    gray = rgb2gray(img_data)a\n",
    "    coor = grid_based_features(gray, 8, 10, 4)\n",
    "    ans.append(coor)\n",
    "\n",
    "df3['Angular Second Moment'] = [sub['Angular Second Moment'] for sub in ans]\n",
    "df3['Maximum Probability'] = [sub['Maximum Probability'] for sub in ans]\n",
    "df3['Inverse Difference Moment'] = [sub['Inverse Difference Moment'] for sub in ans]\n",
    "df3['Contrast'] = [sub['Contrast'] for sub in ans]\n",
    "df3['Entropy'] = [sub['Entropy'] for sub in ans]\n",
    "df3['Correlation'] = [sub['Correlation'] for sub in ans]\n",
    "##normalization\n",
    "for column in df3.columns:\n",
    "    if column == 'labels':\n",
    "        continue\n",
    "    df3[column] = (df3[column] - df3[column].mean())/df3[column].std()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd0ada49",
   "metadata": {},
   "source": [
    "#### Again oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 596,
   "id": "0c095c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_train_y = df3['labels']\n",
    "grid_train_X = df3[features]\n",
    "sampler = SMOTE(sampling_strategy='auto')\n",
    "X_over_grid, y_over_grid = sampler.fit_resample(grid_train_X, grid_train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dcef322",
   "metadata": {},
   "source": [
    "#### Feature extraction for test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 601,
   "id": "4216a714",
   "metadata": {},
   "outputs": [],
   "source": [
    "df4 = pd.DataFrame()\n",
    "labels = np.loadtxt(\"./dataset/test_labels.txt\")\n",
    "##list for class-balancing issues\n",
    "df4['labels'] = labels\n",
    "ans = []\n",
    "for i in range(1,145):\n",
    "    name = 'ts' + str(i) + '.jpg'\n",
    "    image = Image.open(\"./dataset/test\" + '/' + name)\n",
    "    img_data = np.asarray(image)\n",
    "    gray = rgb2gray(img_data)\n",
    "    coor = grid_based_features(gray, 8, 10, 4)\n",
    "    ans.append(coor)\n",
    "\n",
    "df4['Angular Second Moment'] = [sub['Angular Second Moment'] for sub in ans]\n",
    "df4['Maximum Probability'] = [sub['Maximum Probability'] for sub in ans]\n",
    "df4['Inverse Difference Moment'] = [sub['Inverse Difference Moment'] for sub in ans]\n",
    "df4['Contrast'] = [sub['Contrast'] for sub in ans]\n",
    "df4['Entropy'] = [sub['Entropy'] for sub in ans]\n",
    "df4['Correlation'] = [sub['Correlation'] for sub in ans]\n",
    "##normalization\n",
    "for column in df4.columns:\n",
    "    if column == 'labels':\n",
    "        continue\n",
    "    df4[column] = (df4[column] - df4[column].mean())/df4[column].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 603,
   "id": "bc3479d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_y_grid = df4['labels']\n",
    "test_X_grid = df4[features]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce45d64",
   "metadata": {},
   "source": [
    "#### Training SVM with linear kernel (grid-based approach, parameters obtained as a result of tuning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 676,
   "id": "88c83d56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[69 12  7]\n",
      " [ 5 70 13]\n",
      " [16  4 68]]\n",
      "[0.78409091 0.79545455 0.77272727]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.77      0.78      0.78        88\n",
      "         2.0       0.81      0.80      0.80        88\n",
      "         3.0       0.77      0.77      0.77        88\n",
      "\n",
      "    accuracy                           0.78       264\n",
      "   macro avg       0.78      0.78      0.78       264\n",
      "weighted avg       0.78      0.78      0.78       264\n",
      "\n"
     ]
    }
   ],
   "source": [
    "linear_kernel_grid = svm.SVC(C=20, kernel='linear')\n",
    "linear_kernel_grid.fit(X_over_grid, y_over_grid)\n",
    "linear_pred_grid = linear_kernel_grid.predict(X_over_grid)\n",
    "mat = metrics.confusion_matrix(y_over_grid, linear_pred_grid)\n",
    "print(mat)\n",
    "print(mat.diagonal()/mat.sum(axis=1))\n",
    "print(metrics.classification_report(y_over_grid, linear_pred_grid))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4bc0a38",
   "metadata": {},
   "source": [
    "#### SVM with RBF kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 684,
   "id": "b8aa2bc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[80  5  3]\n",
      " [ 8 74  6]\n",
      " [13  2 73]]\n",
      "[0.90909091 0.84090909 0.82954545]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.79      0.91      0.85        88\n",
      "         2.0       0.91      0.84      0.88        88\n",
      "         3.0       0.89      0.83      0.86        88\n",
      "\n",
      "    accuracy                           0.86       264\n",
      "   macro avg       0.87      0.86      0.86       264\n",
      "weighted avg       0.87      0.86      0.86       264\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rbf_kernel_grid = svm.SVC(C=2300, gamma=0.007, kernel='rbf')\n",
    "rbf_kernel_grid.fit(X_over_grid, y_over_grid)\n",
    "rbf_pred_grid = rbf_kernel_grid.predict(X_over_grid)\n",
    "mat = metrics.confusion_matrix(y_over_grid, rbf_pred_grid)\n",
    "print(mat)\n",
    "print(mat.diagonal()/mat.sum(axis=1))\n",
    "print(metrics.classification_report(y_over_grid, rbf_pred_grid))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d23b914f",
   "metadata": {},
   "source": [
    "## Extension 1 (SIFT features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8715df9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5625315",
   "metadata": {},
   "source": [
    "#### Helper function to crop image using the keypoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6a9ca0c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def keypoint_features(img, binNumber, d, coor):\n",
    "    lenx = len(img)\n",
    "    leny = len(img[0])\n",
    "    ans = []\n",
    "    for i in coor:\n",
    "        crop_x_left = np.round(i[0]) - 16\n",
    "        crop_x_right = np.round(i[0]) + 16\n",
    "        crop_y_up = np.round(i[1]) - 16\n",
    "        crop_y_down = np.round(i[1]) + 16\n",
    "        if crop_x_left < 0 or crop_x_right >= lenx or crop_y_up < 0 or crop_y_down >= leny:\n",
    "            continue\n",
    "        window = img[int(crop_x_left):int(crop_x_right), int(crop_y_up):int(crop_y_down)]\n",
    "        feature = calculateCooccurrenceFeatures(calculateAccumulatedCooccurenceMatrix(window, binNumber, d))\n",
    "        ans.append(feature)\n",
    "    return {'Angular Second Moment': np.array([sub['Angular Second Moment'] for sub in ans]).mean(),\n",
    "            'Maximum Probability': np.array([sub['Maximum Probability'] for sub in ans]).mean(),\n",
    "            'Inverse Difference Moment': np.array([sub['Inverse Difference Moment'] for sub in ans]).mean(),\n",
    "            'Contrast': np.array([sub['Contrast'] for sub in ans]).mean(),\n",
    "            'Entropy': np.array([sub['Entropy'] for sub in ans]).mean(),\n",
    "            'Correlation': np.array([sub['Correlation'] for sub in ans]).mean()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29bc70d8",
   "metadata": {},
   "source": [
    "#### Feature extraction for training data (SIFT keypoints are obtained and used)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8e1e8160",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n"
     ]
    }
   ],
   "source": [
    "df5 = pd.DataFrame()\n",
    "sift = cv2.SIFT_create()\n",
    "labels = np.loadtxt(\"./dataset/training_labels.txt\")\n",
    "##list for class-balancing issues\n",
    "df5['labels'] = labels\n",
    "ans = []\n",
    "for i in range(1,187):\n",
    "    print(i)\n",
    "    name = 'tr' + str(i) + '.jpg'\n",
    "    image = Image.open(\"./dataset/training\" + '/' + name)\n",
    "    img_data = np.asarray(image)\n",
    "    gray = rgb2gray(img_data)\n",
    "    keypoints, descriptors = sift.detectAndCompute(img_data, None)\n",
    "    coor = [key.pt for key in keypoints]\n",
    "    feat = keypoint_features(gray, 8, 10, coor)\n",
    "    ans.append(feat)\n",
    "\n",
    "df5['Angular Second Moment'] = [sub['Angular Second Moment'] for sub in ans]\n",
    "df5['Maximum Probability'] = [sub['Maximum Probability'] for sub in ans]\n",
    "df5['Inverse Difference Moment'] = [sub['Inverse Difference Moment'] for sub in ans]\n",
    "df5['Contrast'] = [sub['Contrast'] for sub in ans]\n",
    "df5['Entropy'] = [sub['Entropy'] for sub in ans]\n",
    "df5['Correlation'] = [sub['Correlation'] for sub in ans]\n",
    "##normalization\n",
    "for column in df5.columns:\n",
    "    if column == 'labels':\n",
    "        continue\n",
    "    df5[column] = (df5[column] - df5[column].mean())/df5[column].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "3ea42e29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>labels</th>\n",
       "      <th>Angular Second Moment</th>\n",
       "      <th>Maximum Probability</th>\n",
       "      <th>Inverse Difference Moment</th>\n",
       "      <th>Contrast</th>\n",
       "      <th>Entropy</th>\n",
       "      <th>Correlation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.093505</td>\n",
       "      <td>1.071023</td>\n",
       "      <td>1.044692</td>\n",
       "      <td>-0.894533</td>\n",
       "      <td>-1.055762</td>\n",
       "      <td>-0.846020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.068121</td>\n",
       "      <td>1.086060</td>\n",
       "      <td>1.068913</td>\n",
       "      <td>-0.948292</td>\n",
       "      <td>-1.049326</td>\n",
       "      <td>-0.870583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.743602</td>\n",
       "      <td>0.838123</td>\n",
       "      <td>0.817733</td>\n",
       "      <td>-0.767426</td>\n",
       "      <td>-0.786393</td>\n",
       "      <td>-0.781005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.111899</td>\n",
       "      <td>0.046290</td>\n",
       "      <td>0.012772</td>\n",
       "      <td>0.009999</td>\n",
       "      <td>-0.046942</td>\n",
       "      <td>-0.031371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.651386</td>\n",
       "      <td>-0.698687</td>\n",
       "      <td>-0.749010</td>\n",
       "      <td>0.744767</td>\n",
       "      <td>0.712330</td>\n",
       "      <td>0.526621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>3.0</td>\n",
       "      <td>-0.306867</td>\n",
       "      <td>-0.165079</td>\n",
       "      <td>-0.106896</td>\n",
       "      <td>-0.133058</td>\n",
       "      <td>0.130016</td>\n",
       "      <td>-0.180888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.070991</td>\n",
       "      <td>0.170553</td>\n",
       "      <td>0.200525</td>\n",
       "      <td>-0.286816</td>\n",
       "      <td>-0.157317</td>\n",
       "      <td>-0.312175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>3.0</td>\n",
       "      <td>-0.222961</td>\n",
       "      <td>-0.044212</td>\n",
       "      <td>0.006218</td>\n",
       "      <td>-0.198211</td>\n",
       "      <td>0.043873</td>\n",
       "      <td>-0.281362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.083610</td>\n",
       "      <td>0.249245</td>\n",
       "      <td>0.267277</td>\n",
       "      <td>-0.379272</td>\n",
       "      <td>-0.223320</td>\n",
       "      <td>-0.472820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>3.0</td>\n",
       "      <td>-0.306763</td>\n",
       "      <td>-0.211222</td>\n",
       "      <td>-0.182210</td>\n",
       "      <td>0.027154</td>\n",
       "      <td>0.207203</td>\n",
       "      <td>-0.058969</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>186 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     labels  Angular Second Moment  Maximum Probability  \\\n",
       "0       1.0               1.093505             1.071023   \n",
       "1       1.0               1.068121             1.086060   \n",
       "2       1.0               0.743602             0.838123   \n",
       "3       1.0               0.111899             0.046290   \n",
       "4       1.0              -0.651386            -0.698687   \n",
       "..      ...                    ...                  ...   \n",
       "181     3.0              -0.306867            -0.165079   \n",
       "182     3.0               0.070991             0.170553   \n",
       "183     3.0              -0.222961            -0.044212   \n",
       "184     3.0               0.083610             0.249245   \n",
       "185     3.0              -0.306763            -0.211222   \n",
       "\n",
       "     Inverse Difference Moment  Contrast   Entropy  Correlation  \n",
       "0                     1.044692 -0.894533 -1.055762    -0.846020  \n",
       "1                     1.068913 -0.948292 -1.049326    -0.870583  \n",
       "2                     0.817733 -0.767426 -0.786393    -0.781005  \n",
       "3                     0.012772  0.009999 -0.046942    -0.031371  \n",
       "4                    -0.749010  0.744767  0.712330     0.526621  \n",
       "..                         ...       ...       ...          ...  \n",
       "181                  -0.106896 -0.133058  0.130016    -0.180888  \n",
       "182                   0.200525 -0.286816 -0.157317    -0.312175  \n",
       "183                   0.006218 -0.198211  0.043873    -0.281362  \n",
       "184                   0.267277 -0.379272 -0.223320    -0.472820  \n",
       "185                  -0.182210  0.027154  0.207203    -0.058969  \n",
       "\n",
       "[186 rows x 7 columns]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d348be10",
   "metadata": {},
   "source": [
    "#### Oversampling again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a6edba35",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [col for col in df5.columns if col != 'labels']\n",
    "SIFT_train_y = df5['labels']\n",
    "SIFT_train_X = df5[features]\n",
    "sampler = SMOTE(sampling_strategy='auto')\n",
    "X_over_SIFT, y_over_SIFT = sampler.fit_resample(SIFT_train_X, SIFT_train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef2cbb5b",
   "metadata": {},
   "source": [
    "#### Same process for test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d1455571",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n"
     ]
    }
   ],
   "source": [
    "df6 = pd.DataFrame()\n",
    "labels = np.loadtxt(\"./dataset/test_labels.txt\")\n",
    "##list for class-balancing issues\n",
    "df6['labels'] = labels\n",
    "ans = []\n",
    "for i in range(1,145):\n",
    "    print(i)\n",
    "    name = 'ts' + str(i) + '.jpg'\n",
    "    image = Image.open(\"./dataset/test\" + '/' + name)\n",
    "    img_data = np.asarray(image)\n",
    "    gray = rgb2gray(img_data)\n",
    "    keypoints, descriptors = sift.detectAndCompute(img_data, None)\n",
    "    coor = [key.pt for key in keypoints]\n",
    "    feat = keypoint_features(gray, 8, 10, coor)\n",
    "    ans.append(feat)\n",
    "\n",
    "df6['Angular Second Moment'] = [sub['Angular Second Moment'] for sub in ans]\n",
    "df6['Maximum Probability'] = [sub['Maximum Probability'] for sub in ans]\n",
    "df6['Inverse Difference Moment'] = [sub['Inverse Difference Moment'] for sub in ans]\n",
    "df6['Contrast'] = [sub['Contrast'] for sub in ans]\n",
    "df6['Entropy'] = [sub['Entropy'] for sub in ans]\n",
    "df6['Correlation'] = [sub['Correlation'] for sub in ans]\n",
    "##normalization\n",
    "for column in df6.columns:\n",
    "    if column == 'labels':\n",
    "        continue\n",
    "    df6[column] = (df6[column] - df6[column].mean())/df6[column].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "64fdac55",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_y_SIFT = df6['labels']\n",
    "test_X_SIFT = df6[features]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60f9a787",
   "metadata": {},
   "source": [
    "#### SVM training with linear kernel (SIFT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "cc534a52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[31  3 14]\n",
      " [ 1 51  5]\n",
      " [15  5 19]]\n",
      "[0.64583333 0.89473684 0.48717949]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.66      0.65      0.65        48\n",
      "         2.0       0.86      0.89      0.88        57\n",
      "         3.0       0.50      0.49      0.49        39\n",
      "\n",
      "    accuracy                           0.70       144\n",
      "   macro avg       0.67      0.68      0.68       144\n",
      "weighted avg       0.70      0.70      0.70       144\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# best = 0\n",
    "# best_c = 0\n",
    "# for c in np.arange(3100, 4200, 100):\n",
    "#     linear_kernel = svm.SVC(C=c, kernel='linear')\n",
    "#     score = cross_val_score(linear_kernel, X_over_SIFT, y_over_SIFT, cv=10).mean()\n",
    "#     if score > best:\n",
    "#         best = score\n",
    "#         best_c = c\n",
    "        \n",
    "# print(best, best_c)\n",
    "linear_kernel_SIFT = svm.SVC(C=4100, kernel='linear')\n",
    "linear_kernel_SIFT.fit(X_over_SIFT, y_over_SIFT)\n",
    "linear_pred_SIFT = linear_kernel_SIFT.predict(test_X_SIFT)\n",
    "mat = metrics.confusion_matrix(test_y_SIFT, linear_pred_SIFT)\n",
    "print(mat)\n",
    "print(mat.diagonal()/mat.sum(axis=1))\n",
    "print(metrics.classification_report(test_y_SIFT, linear_pred_SIFT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "80013978",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[41  1  6]\n",
      " [ 2 51  4]\n",
      " [ 3  9 27]]\n",
      "[0.85416667 0.89473684 0.69230769]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.89      0.85      0.87        48\n",
      "         2.0       0.84      0.89      0.86        57\n",
      "         3.0       0.73      0.69      0.71        39\n",
      "\n",
      "    accuracy                           0.83       144\n",
      "   macro avg       0.82      0.81      0.82       144\n",
      "weighted avg       0.83      0.83      0.83       144\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rbf_kernel_SIFT = svm.SVC(C=5000, gamma=0.007, kernel='rbf')\n",
    "rbf_kernel_SIFT.fit(X_over_SIFT, y_over_SIFT)\n",
    "rbf_pred_SIFT = rbf_kernel_SIFT.predict(test_X_SIFT)\n",
    "mat = metrics.confusion_matrix(test_y_SIFT, rbf_pred_SIFT)\n",
    "print(mat)\n",
    "print(mat.diagonal()/mat.sum(axis=1))\n",
    "print(metrics.classification_report(test_y_SIFT, rbf_pred_SIFT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff03ba27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b132f36a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d923d86b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
